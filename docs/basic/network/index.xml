<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s 网络 on Kubernetes 学习笔记</title>
    <link>https://ikingye.github.io/study-k8s/docs/basic/network/</link>
    <description>Recent content in K8s 网络 on Kubernetes 学习笔记</description>
    <generator>Hugo -- gohugo.io</generator>
    
	<atom:link href="https://ikingye.github.io/study-k8s/docs/basic/network/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K8s 端口</title>
      <link>https://ikingye.github.io/study-k8s/docs/basic/network/port/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ikingye.github.io/study-k8s/docs/basic/network/port/</guid>
      <description>K8s 端口 总的来说，port 和 nodePort 都是 service 的端口，前者暴露给集群内客户访问服务，后者暴露给集群外客户访问服务。从这两个端口到来的数据都需要经过反向代理 kube-proxy 流入后端 pod 的 targetPod，从而到达 pod 上的容器内。
pod 模板 作用类似于 docker -p 选项 containerPort: 容器需要暴露的端口 hostPort: 容器暴露的端口映射到的主机端口
service (ClusterIP) port: service 中 clusterIP 对应的端口 targetPort: clusterIP 作为负载均衡， 后端目标实例 (容器) 的端口
service (NodePort) nodePort: cluster ip 只能集群内部访问 (源与目标需要满足两个条件: kube-proxy 正常运行，跨主机容器网络通信正常)， nodePort 会在每个 kubelet 节点的宿主机开启一个端口，用于应用集群外部访问
nodePort 到 clusterIP 的映射，是 kube-proxy 实现的。</description>
    </item>
    
    <item>
      <title>KubeDNS</title>
      <link>https://ikingye.github.io/study-k8s/docs/basic/network/kubedns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ikingye.github.io/study-k8s/docs/basic/network/kubedns/</guid>
      <description>KubeDNS 在 Linux 系统中，/etc/resolv.conf 是存储 DNS 服务器的文件， 普通 Pod 的 /etc/resolv.conf 文件应该存储的是 kube-dns 的 Service IP。
nameserver 10.99.0.2 # 这里存储的是kube-dns的Service IP search default.svc.cluster.local. svc.cluster.local. cluster.local. options ndots:5 k8s 中域名是如何被解析的 在 k8s 中，一个 Pod 如果要访问相同同 Namespace 下的 Service（比如 user-svc），那么只需要 curl user-svc。 如果 Pod 和 Service 不在同一域名下，那么就需要在 Service Name 之后添加上 Service 所在的 Namespace（比如 beta），curl user-svc.beta。 那么 k8s 是如何知道这些域名是内部域名并为他们做解析的呢？
无论是在 宿主机 或者是在 k8s 集群中，DNS 解析会依赖这个三个文件
 /etc/host.conf /etc/hosts /etc/resolv.conf  /etc/resolv.conf resolv.conf 是 Pod 在 dnsPolicy: ClusterFirst 的情况下，k8s 为其自动生成的。 在该 Pod 内请求的所有的域名解析都需要经过 DNS Service 进行解析，不管是集群内部域名还是外部域名。</description>
    </item>
    
    <item>
      <title>CoreDNS</title>
      <link>https://ikingye.github.io/study-k8s/docs/basic/network/coredns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ikingye.github.io/study-k8s/docs/basic/network/coredns/</guid>
      <description>CoreDNS CoreDNS vs KubeDNS 在 kube-dns 中，一个 pod 内使用了数个容器：kubedns、dnsmasq 和 sidecar。
 kubedns 容器监视 Kubernetes API 并基于 Kubernetes DNS 规范提供 DNS 记录， dnsmasq 提供缓存和存根域支持， sidecar 提供指标和健康检查。  此设置会导致一些问题随着时间的推移而出现。首先，dnsmasq 中的安全漏洞导致过去需要发布 Kubernetes 安全补丁。 此外，由于 dnsmasq 处理存根域，但 kubedns 处理 External Services，因此你无法在外部服务中使用存根域，这非常限制该功能（参阅 dns＃131）。
在 CoreDNS 中，所有这些功能都在一个容器中完成 —— 该容器运行用 Go 编写的进程。 启用的不同插件来复制（并增强）kube-dns 中的功能。</description>
    </item>
    
  </channel>
</rss>